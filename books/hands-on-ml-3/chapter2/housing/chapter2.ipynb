{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b40f35f",
   "metadata": {},
   "source": [
    "# Chapter 2 : End-To-End Machine Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb19502",
   "metadata": {},
   "source": [
    "### Our approach for this problem will be based below main steps-\n",
    "\n",
    "1. Look at the bigger picture\n",
    "2. Get the data\n",
    "3. Explore & visualise the data to gain insights\n",
    "4. Prepare the data for machine learning algorithms\n",
    "5. Select a model & train it\n",
    "6. Fine-tune your model\n",
    "7. Present your solution\n",
    "8. Launch, monitor & maintain your system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785486f7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657844ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import numpy as np\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector, make_column_transformer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecc46e1",
   "metadata": {},
   "source": [
    "## 2. Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cd6565",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.getcwd(), \"housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab5136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec1024",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = pd.read_csv(data_path)\n",
    "housing_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5568831",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70378c66",
   "metadata": {},
   "source": [
    "Notice feature 'total_bedrooms' has only 20433 out of total dataset entries 20640. Meaning 207 districts does not have valid value for this feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cd3928",
   "metadata": {},
   "source": [
    "### Exploring 'ocean_proximity' as categorical feature: 5 categorical values as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1248b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df['ocean_proximity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecc45b8",
   "metadata": {},
   "source": [
    "### Exploring the distribution of numerical attributes\n",
    "- Numerical features are pretty skewed, we need may need to normalise them\n",
    "- Few features like housing_median_age & median_house_value are capped at different levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0aaae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.hist(figsize=(15,8), bins=40) \n",
    "# figsize = how stretch the figure will be\n",
    "# bins = how smooth the distribution looks like, divide data into smaller bins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef231241",
   "metadata": {},
   "source": [
    "### Creating a Test Set\n",
    "\n",
    "- Here we have randomly selected the test set which is not the ideal way because it might introduce the sampling bias. Test set should be the representative of all the dataset to produce reliable & accurate results.\n",
    "- Assuming 'median_income' feature is very important in predicting 'median_house_value' so the test set should be representative of this feature.\n",
    "- Looking at 'median_income', it appears to be divided mostly from 1.5 to 6. Let's create new feature 'income_cat'to represent categories for this continous variable. Categories shouldn't be too large or small\n",
    "- pd.cut() function provides the way to create these categories\n",
    "- train_test_split() function provides an argument stratify=feature_name which is used to create test set with stratified strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b12eed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df[\"income_cat\"] = pd.cut(housing_df[\"median_income\"],\n",
    "                                 bins=[0.,1.5,3.,4.5,6., np.inf],\n",
    "                                 labels=[1,2,3,4,5]) # np.inf required as bins should > labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716fb710",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df[\"income_cat\"].value_counts().sort_index().plot.bar(rot=0, grid=True) # rot: \n",
    "plt.xlabel(\"Income category\")\n",
    "plt.ylabel(\"Number of districts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef11f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstratified data\n",
    "unstrat_train_set, unstrat_test_set = train_test_split(housing_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2997f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified sampling\n",
    "strat_train_set, strat_test_set = train_test_split(housing_df, test_size=0.2, random_state=42, stratify=housing_df[\"income_cat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c048613",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set[\"income_cat\"].value_counts()/len(strat_test_set) # total strat test set = 4128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393fd325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we won't use the income_cat feature again & also don't want it to influence our predictions, so we will remove it\n",
    "for feat_set in (strat_train_set, strat_test_set):\n",
    "    feat_set.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f621e37a",
   "metadata": {},
   "source": [
    "## 3. Explore & Visualise the Data to Gain Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e5cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets' copy the original data to analyse/manipulate\n",
    "\n",
    "housing = strat_train_set.copy()\n",
    "housing.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b36476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we are dealing with geographical data so it make sense to plot the scatterplot\n",
    "\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.2,\n",
    "            grid=True, s=housing[\"population\"]/100, \n",
    "              c=\"median_house_value\",\n",
    "            figsize=(10,8)) # alpha represents high density areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5406c9b6",
   "metadata": {},
   "source": [
    "### Correlations of attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de71e676-e6f9-4c4f-8ab8-c45e3c9fdb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daae0086",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr(numeric_only=True) # corr method uses Pearson' coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb12ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db02bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way to look the correlations between numerical attributes is scatter plots\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12,8))\n",
    "plt.show()\n",
    "# only promising attribute to predict median_house_value would be median_income"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98275ab-cd09-4559-9147-3fe224d40443",
   "metadata": {},
   "source": [
    "### Experiment with Attribute Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ee0dda-3d9c-4ff5-8e2c-8d0080e067c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"rooms_per_house\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_ratio\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"people_per_house\"] = housing[\"population\"]/housing[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846fbc7e-5db0-4c6a-be31-52e9e63d485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c34b6a-d454-4ed3-969e-cde6913b4441",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf113d17-ea03-4251-85de-2e041c6a7790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here bedrooms_ratio is negatively correlated to median_house_value\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc99ebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets zoom in with median_income\n",
    "housing.plot(kind=\"scatter\",x=\"median_income\", y=\"median_house_value\",alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa13515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the predictors from data\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) # axis = 1 means transform via columns\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "# adding the numeric features again to housing dataframe\n",
    "housing[\"rooms_per_house\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_ratio\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"people_per_house\"] = housing[\"population\"]/housing[\"households\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b691ca67",
   "metadata": {},
   "source": [
    "### Clean the Data\n",
    "- Most ML algorithms won't work on missing data so we need to handle this data\n",
    "- Its good idea to use imputer from sklearn\n",
    "    - You can apply the transformation on all the data simultanously like train, test, validation\n",
    "    - It stores the missing value strategy (for example, median) for all the features and for incoming data\n",
    "    - Imputer saves the result into its 'statistics_' attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf79475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median can only work with numeric data features\n",
    "imputer = SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd4a2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with numeric only features\n",
    "housing_num = housing.select_dtypes(include=[np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41944b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.info() # total_bedrooms have null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691289c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b3356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputer saves the result in its 'statistics_' instance. Showing median for all 8 numeric features\n",
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f8ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now using this trained imputer to transform the values in training set\n",
    "housing_imputed = imputer.transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c550c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(housing_imputed, type(housing_imputed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8461f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframe from numpy array which is the output of transformer\n",
    "housing_tr = pd.DataFrame(housing_imputed, columns=housing_num.columns, index=housing_num.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cdbc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are no null values left\n",
    "housing_tr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31c7334",
   "metadata": {},
   "source": [
    "### Handling categorical attributes\n",
    "- Currently we only have the 'ocean_proximity' attribute as categorical\n",
    "- Simple way is to use OrdinalEncoder but the problem with this approach is it creates importance within nearer values. For example, categorised 1,2,3 values works with categories like good, better, excellent etc but not with ocean_proximity.\n",
    "- Better and common solution is to use one hot encoding where each category creates an attribute with 1(hot) or 0 (cold) values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a2780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat = housing[[\"ocean_proximity\"]] # housing[\"ocean_proximity\"] -> series, housing[[\"ocean_proximity\"]] -> dataframe(housing_cat)\n",
    "\n",
    "housing_cat.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7cda3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cc638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd67a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb7f98b",
   "metadata": {},
   "source": [
    "### Feature Scaling & transformation\n",
    "\n",
    "- Machine learning algorithms don't perform well if the features are not scaled properly. For example, median_income range from 0-15 and total number of rooms range from 6 to 39320. In this case the algorithm will be baised towards 'total num of rooms' feature.\n",
    "- sklearn provides MinMaxScaling, StandardScaler options to scale numeric features. MinMaxScaling squeeze feature values from 0 to 1 while StandardScaler substract mean & divide by standard deviation\n",
    "- rbf kernels are radial basis function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ec9dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scalar = MinMaxScaler()\n",
    "housing_minmax = minmax_scalar.fit_transform(housing_num)\n",
    "housing_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ac41d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scalar = StandardScaler()\n",
    "housing_std_scalar = std_scalar.fit_transform(housing_num)\n",
    "housing_std_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a063d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_simil_35 = rbf_kernel(housing[[\"housing_median_age\"]], [[35]], gamma=0.1)\n",
    "age_simil_35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38248e8-1452-4054-908c-c07391b6a59e",
   "metadata": {},
   "source": [
    "### Transformation Pipeline\n",
    "\n",
    "- sklearn provides a great functionality of transformers to create pipelines of functions\n",
    "- transformers can be used to hook multiple preprocessing steps into one. Output of one step will feed into as input of next one and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87624e0f-261d-4c41-b9da-5544c7563e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a pipeline which handles numeric features - imputation & scaling\n",
    "num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26601af-eef4-407b-8ce3-a238fdc73ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num_prepared = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9646fc-db71-44ca-98df-7adae4e37eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is ndarray\n",
    "housing_num_prepared[:2].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d84887-fd8f-4706-ae7c-022dc7198def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of columns\n",
    "num_attributes = list(num_pipeline.get_feature_names_out())\n",
    "num_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d357f453-82d9-4946-bc64-ac436f362755",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing_num_prepared = pd.DataFrame(housing_num_prepared, columns=num_pipeline.get_feature_names_out(), index=housing_num.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6523ef1d-0b3d-4d6c-97a4-ab9d8294bb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing_num_prepared.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340cd126-22f6-43f8-88d2-7095769e9b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0ee492-4d8b-4a73-960f-ae7378aa3c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_attributes = list(housing_cat.columns)\n",
    "cat_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15621cf5-1a7a-40b5-86af-7a5c9ec716d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly create a pipeline which handles categorical values - imputation & onehotencoding\n",
    "cat_pipeline = make_pipeline(SimpleImputer(strategy=\"most_frequent\"),OneHotEncoder(handle_unknown=\"ignore\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4113a3ce-5dfb-4794-af45-46dfe10d4ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets combine numeric & categorical features into one\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attributes),\n",
    "    (\"cat\", cat_pipeline, cat_attributes)\n",
    "])\n",
    "\n",
    "# instead of above approach you can do following steps to get the same output with much ease\n",
    "# preprocessing = make_column_transformer(num_pipeline, make_column_selector(dtype_include=np.number),\n",
    "#                                        cat_pipeline, make_column_selector(dtype_include=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9367835-d677-4955-9e90-4461ef5de603",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "housing_prepared = preprocessing.fit_transform(housing)\n",
    "housing_prepared[:5].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcf34b9-3efa-491c-82d6-1cbebc85b3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_prepared.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed8c3b5-1553-44d9-84c4-dca1e83e7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7873f0b-a2d6-4112-b0c3-9d11d887a7cb",
   "metadata": {},
   "source": [
    "#### You can combine all preprocessing steps into one column transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1250f1bf-5820-47c9-95ac-2de6f21f4465",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_df = pd.DataFrame(housing_prepared, columns=preprocessing.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaae88ae-6ff4-4df0-a6c9-1ecb15877236",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_df.hist(figsize=(15,8), bins=40);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f79022e-2c51-4fa4-b116-ef82fcccd91e",
   "metadata": {},
   "source": [
    "## Select and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7311fc-e66b-41c4-82f5-f2aae72e070b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_juypter",
   "language": "python",
   "name": "venv_juypter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
